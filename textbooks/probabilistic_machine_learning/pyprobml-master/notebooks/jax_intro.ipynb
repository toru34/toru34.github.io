{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "jax_intro.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "DeXQ1pOcmhKu",
        "D2dSgG-cnVIe",
        "riXmT3Iok_yq",
        "QuMHSd3wr1xH",
        "kUj-VuSzmFaV",
        "VTIybB8b4ar0",
        "nglie5m7q607",
        "bBMcsg4uoKua",
        "mg9ValMRm_Md",
        "MeoGcnV54YY9",
        "EOzQJSX3JOF9",
        "ZF_OjIRrisDB",
        "AKTg9m9yz7Ib",
        "ZMZVMEOPbLWt",
        "TdtqCDWZAC2f",
        "A-L-dB_GBWNq",
        "YFsfzYHzhbM3",
        "QGxWcHWJKtsq",
        "o8iMrvUXK8Id",
        "8-wtZiZmU329",
        "Ru5D0tZEiV2e"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/probml/pyprobml/blob/master/notebooks/jax_intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4bE-S8yDALH"
      },
      "source": [
        "# Brief introduction to JAX \n",
        "\n",
        "murphyk@gmail.com.\n",
        "\n",
        "[JAX](https://github.com/google/jax) is a  version of NumPy that runs fast on CPU, GPU and TPU, by compiling down to XLA. It also has an excellent automatic differentiation library, extending the earlier [autograd](https://github.com/hips/autograd) package, which makes it easy to compute higher order derivatives, gradients of complex functions (e.g., optimize an iterative solver), etc.\n",
        "The JAX interface is almost identical to NumPy (by design), but with some small differences, and lots of additional features.\n",
        "We give a brief introduction below. \n",
        "For more details, see [this list of JAX tutorials](https://github.com/probml/pyprobml/blob/master/notebooks/jax_tutorials.md).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvVqju_6BE5c"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCI0G3tfDFSs"
      },
      "source": [
        "# Standard Python libraries\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "from functools import partial\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "np.set_printoptions(precision=3)\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL\n",
        "import imageio\n",
        "\n",
        "from typing import Tuple, NamedTuple\n",
        "\n",
        "from IPython import display\n",
        "%matplotlib inline\n",
        "\n",
        "import sklearn\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9kAsUWYDIOk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4568e80c-e8a5-4a81-ecb0-20a2d994b0b1"
      },
      "source": [
        "\n",
        "# Load JAX\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "from jax import random, vmap, jit, grad, value_and_grad, hessian, jacfwd, jacrev\n",
        "print(\"jax version {}\".format(jax.__version__))\n",
        "# Check the jax backend\n",
        "print(\"jax backend {}\".format(jax.lib.xla_bridge.get_backend().platform))\n",
        "key = random.PRNGKey(0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "jax version 0.2.9\n",
            "jax backend gpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BSjMVv-bctU"
      },
      "source": [
        "%%capture\n",
        "!pip install git+https://github.com/deepmind/dm-haiku\n",
        "import haiku as hk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEhWCbQibkn1"
      },
      "source": [
        "%%capture\n",
        "!pip install --upgrade -q git+https://github.com/google/flax.git\n",
        "import flax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLfVevcW0F4U"
      },
      "source": [
        "# Hardware accelerators\n",
        "\n",
        "Colab makes it easy to use GPUs and TPUs for speeding up some workflows, especially related to deep learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9saSIGTn661X"
      },
      "source": [
        "## GPUs\n",
        "\n",
        "Colab offers graphics processing units (GPUs) which can be much faster than CPUs (central processing units), as we illustrate below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXExOyfluzIj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5015686b-5569-4215-9b98-bf66448690f2"
      },
      "source": [
        "# Check if GPU is available and its model, memory ...etc.\n",
        "!nvidia-smi\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Feb  9 13:54:21 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.39       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P0    40W / 300W |  14549MiB / 16130MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c4Y-hI3FR8f",
        "outputId": "ac786de8-28b1-4621-d4b5-5adf7d3d9d97"
      },
      "source": [
        "\n",
        "# Check if JAX is using GPU\n",
        "print(\"jax backend {}\".format(jax.lib.xla_bridge.get_backend().platform))\n",
        "# Check the devices avaiable for JAX\n",
        "jax.devices()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "jax backend gpu\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[GpuDevice(id=0)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ri2ZN_C7ul5"
      },
      "source": [
        "Let's see how JAX can speed up things like matrix-matrix multiplication.\n",
        "\n",
        "First the numpy/CPU version."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UxKzydkDQ8-"
      },
      "source": [
        "# Parameters for the experiment\n",
        "size = int(1e3)\n",
        "number_of_loops=int(1e2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmSu22WZ7nRw"
      },
      "source": [
        "# Standard numpy CPU\n",
        "\n",
        "def f(x=None):\n",
        "  if not isinstance(x, np.ndarray):\n",
        "    x=np.ones((size, size), dtype=np.float32) \n",
        "  return np.dot(x, x.T)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IEG-7grMDY1Z",
        "outputId": "b4cb7328-797b-4d59-9c5d-178d774beabb"
      },
      "source": [
        "%timeit -o -n $number_of_loops f()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100 loops, best of 3: 14.9 ms per loop\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TimeitResult : 100 loops, best of 3: 14.9 ms per loop>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-y3VO5oc-PhO",
        "outputId": "9700fb8e-dd77-4760-b612-77ad2e34819b"
      },
      "source": [
        "res = _ # get result of last cell\n",
        "time_cpu = res.best\n",
        "print(time_cpu)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.014857908460000999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJPBadZ88Hmi"
      },
      "source": [
        "Now we look at the JAX version. JAX supports execution on [XLA](https://www.tensorflow.org/xla) devices, which can be CPU, GPU or even TPU. We added that block_until_ready because JAX uses [asynchronous execution](https://jax.readthedocs.io/en/latest/async_dispatch.html) by default.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ptXbto48AWd"
      },
      "source": [
        "# JAX device execution\n",
        "# https://github.com/google/jax/issues/1598\n",
        "\n",
        "def jf(x=None): \n",
        "  if not isinstance(x, jnp.ndarray):\n",
        "    x=jnp.ones((size, size), dtype=jnp.float32)\n",
        "  return jnp.dot(x, x.T)\n",
        "\n",
        "\n",
        "f_gpu = jit(jf, backend='gpu')\n",
        "f_cpu = jit(jf, backend='cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVMO-3wPDxoV",
        "outputId": "36d2766c-723b-4e82-ed87-187e7efc7817"
      },
      "source": [
        "# Time the CPU version\n",
        "\n",
        "%timeit -o -n $number_of_loops f_cpu() "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100 loops, best of 3: 14.5 ms per loop\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TimeitResult : 100 loops, best of 3: 14.5 ms per loop>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWQR_9QMDzcd",
        "outputId": "6cdb9fc4-23d0-44dc-8847-1e78cf516376"
      },
      "source": [
        "res = _\n",
        "time_jcpu = res.best\n",
        "print(time_jcpu)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.014495725080000738\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPj1zQ4BD3Mq",
        "outputId": "112b5149-8753-41d9-da56-cd8af782f6f5"
      },
      "source": [
        "# Time the GPU version\n",
        "\n",
        "%timeit -o -n $number_of_loops f_gpu().block_until_ready() "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The slowest run took 44.47 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "100 loops, best of 3: 276 µs per loop\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TimeitResult : 100 loops, best of 3: 276 µs per loop>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTWn8YMMEWfH",
        "outputId": "6cf4820d-81eb-4117-ab3e-f4ab386d1755"
      },
      "source": [
        "res = _\n",
        "time_jgpu = res.best\n",
        "print(time_jgpu)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0002756696500000544\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVLUmoB_9pPV",
        "outputId": "f886a60a-6bff-4a87-9af1-5f6717865ec6"
      },
      "source": [
        "print('JAX CPU time {:0.6f}, Numpy CPU time {:0.6f}, speedup {:0.6f}'.format(\n",
        "    time_jcpu, time_cpu, time_cpu/time_jcpu))\n",
        "print('JAX GPU time {:0.6f}, JAX CPU time {:0.6f}, speedup {:0.6f}'.format(\n",
        "    time_jgpu, time_jcpu, time_jcpu/time_jgpu))\n",
        "print('JAX GPU time {:0.6f}, Numpy CPU time {:0.6f}, speedup {:0.6f}'.format(\n",
        "    time_jgpu, time_cpu, time_cpu/time_jgpu))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "JAX CPU time 0.014496, Numpy CPU time 0.014858, speedup 1.024986\n",
            "JAX GPU time 0.000276, JAX CPU time 0.014496, speedup 52.583682\n",
            "JAX GPU time 0.000276, Numpy CPU time 0.014858, speedup 53.897513\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReLo-XAQEcNN"
      },
      "source": [
        "This illustrates the power of XLA (Accelerated Linear Algebra compiler), even for apples to apples comparison of same hardware. JAX can be faster even on a CPU than the standard numpy. Note that due to various factors exectution times can vary per run even on a single machine, however the overall performance of human writtten vs. compiler emitted (i.e. numpy C backend vs. JAX XLA jit backend) is a topic of its own."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BO1H-_vSCz8r"
      },
      "source": [
        "We can move numpy arrays to the GPU for speed. The result will be transferred back to CPU for printing, saving, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpxRFku_C9hR",
        "outputId": "a7149137-a6bd-4203-cfec-d0d82e6d63ea"
      },
      "source": [
        "from jax import device_put\n",
        "\n",
        "x = np.ones((size, size)).astype(np.float32)\n",
        "print(type(x))\n",
        "%timeit -o -n $number_of_loops f(x)\n",
        "\n",
        "x = device_put(x)\n",
        "print(type(x))\n",
        "%timeit -o -n $number_of_loops jf(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "100 loops, best of 3: 14.1 ms per loop\n",
            "<class 'jax.interpreters.xla._DeviceArray'>\n",
            "100 loops, best of 3: 639 µs per loop\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TimeitResult : 100 loops, best of 3: 639 µs per loop>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WS3g8MKl0U9B"
      },
      "source": [
        "## TPUs\n",
        "\n",
        "We can turn on the tensor processing unit as shown below.\n",
        "Everything else \"just works\" as before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXVaclfH0Xxg"
      },
      "source": [
        "import jax.tools.colab_tpu\n",
        "jax.tools.colab_tpu.setup_tpu()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Zsh5DdOF4R1"
      },
      "source": [
        "# Vmap <a class=\"anchor\" id=\"vmap\"></a>\n",
        "\n",
        "We often write a function to process a single vector or matrix, and then want to apply it to a batch of data. Using for loops is slow, and manually batchifying code is complex. Fortunately we can use the `vmap` function, which will map our function across a set of inputs, automatically batchifying it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeXQ1pOcmhKu"
      },
      "source": [
        "## Example: 1d convolution\n",
        "\n",
        "(This example is from the Deepmind tutorial.)\n",
        "\n",
        "Consider standard 1d convolution of two vectors.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0Ebbw6qmsk8",
        "outputId": "fafbcbb1-034f-40c9-e67a-479fd87e3367"
      },
      "source": [
        "x = jnp.arange(5)\n",
        "w = jnp.array([2., 3., 4.])\n",
        "\n",
        "def convolve(x, w):\n",
        "  output = []\n",
        "  for i in range(1, len(x)-1):\n",
        "    output.append(jnp.dot(x[i-1:i+2], w))\n",
        "  return jnp.array(output)\n",
        "\n",
        "convolve(x, w)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([11., 20., 29.], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-34EQoIsmx8m"
      },
      "source": [
        "Now suppose we want to convolve multiple vectors with multiple kernels. The simplest way is to use a for loop, but this is slow.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8NJ-ZQ3m3-K",
        "outputId": "ba7434b9-02f6-43b0-b30d-70a19704f9d4"
      },
      "source": [
        "xs = jnp.stack([x, x])\n",
        "ws = jnp.stack([w, w])\n",
        "\n",
        "def manually_batched_convolve(xs, ws):\n",
        "  output = []\n",
        "  for i in range(xs.shape[0]):\n",
        "    output.append(convolve(xs[i], ws[i]))\n",
        "  return jnp.stack(output)\n",
        "\n",
        "manually_batched_convolve(xs, ws)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([[11., 20., 29.],\n",
              "             [11., 20., 29.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQzNQCpGnGEu"
      },
      "source": [
        "We can manually vectorize the code, but it is complex."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1TUBdS1nJD7",
        "outputId": "675c4abe-f72a-4782-9aff-4d450969d2a8"
      },
      "source": [
        "def manually_vectorised_convolve(xs, ws):\n",
        "  output = []\n",
        "  for i in range(1, xs.shape[-1] -1):\n",
        "    output.append(jnp.sum(xs[:, i-1:i+2] * ws, axis=1))\n",
        "  return jnp.stack(output, axis=1)\n",
        "\n",
        "manually_vectorised_convolve(xs, ws)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([[11., 20., 29.],\n",
              "             [11., 20., 29.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SE-gk4ZnNLS"
      },
      "source": [
        "Fortunately vmap can do this for us!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEMqO2k7nQdR",
        "outputId": "f34ede8e-28de-4031-a2cf-92013733f2a9"
      },
      "source": [
        "auto_batch_convolve = jax.vmap(convolve)\n",
        "\n",
        "auto_batch_convolve(xs, ws)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([[11., 20., 29.],\n",
              "             [11., 20., 29.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2dSgG-cnVIe"
      },
      "source": [
        "## Axes\n",
        "\n",
        "By default, vmap vectorizes over the first axis of each of its inputs. If the first argument has a batch and the second does not, ,specify `in_axes=[0,None]`, so the second argument is not vectorized over."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OuK4kBGsnkS-",
        "outputId": "8aad47fb-f0f4-44fc-bf33-b8a2c2b7f67c"
      },
      "source": [
        "jax.vmap(convolve, in_axes=[0, None])(xs, w)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([[11., 20., 29.],\n",
              "             [11., 20., 29.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8yq14nqoYXX"
      },
      "source": [
        "We can also vectorize over other dimensions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aRLJNGQoctn",
        "outputId": "5e7fb6ac-d7fe-42a7-cd2c-acbe40fd6e98"
      },
      "source": [
        "\n",
        "print(xs.shape)\n",
        "xst = jnp.transpose(xs)\n",
        "print(xst.shape)\n",
        "\n",
        "wst = jnp.transpose(ws)\n",
        "\n",
        "auto_batch_convolve_v2 = jax.vmap(convolve, in_axes=1, out_axes=1)\n",
        "auto_batch_convolve_v2(xst, wst)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2, 5)\n",
            "(5, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([[11., 11.],\n",
              "             [20., 20.],\n",
              "             [29., 29.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riXmT3Iok_yq"
      },
      "source": [
        "## Example: logistic regression\n",
        "\n",
        "We now give another example, using binary logistic regression.\n",
        "Let us start with a predictor for a single example\n",
        "."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XAMcxMsF0-Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "28f3f184-ce35-4f41-84aa-35ad0bf4cb23"
      },
      "source": [
        "\n",
        "D = 2\n",
        "N = 3\n",
        "\n",
        "w = np.random.normal(size=(D,))\n",
        "X = np.random.normal(size=(N,D))\n",
        "\n",
        "def sigmoid(x): return 0.5 * (jnp.tanh(x / 2.) + 1)\n",
        "\n",
        "def predict_single(x):\n",
        "    return sigmoid(jnp.dot(w, x)) # <(D) , (D)> = (1) # inner product\n",
        "  \n",
        "print(predict_single(X[0,:])) # works\n",
        "\n",
        "print(predict_single(X)) # fails"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.2391612\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-eb94977c6c56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# works\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# fails\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-eb94977c6c56>\u001b[0m in \u001b[0;36mpredict_single\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# <(D) , (D)> = (1) # inner product\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# works\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36mdot\u001b[0;34m(a, b, precision)\u001b[0m\n\u001b[1;32m   3355\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3356\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_max\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_ndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_ndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3357\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3359\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mb_ndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/_src/lax/lax.py\u001b[0m in \u001b[0;36mdot\u001b[0;34m(lhs, rhs, precision)\u001b[0m\n\u001b[1;32m    632\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m     raise TypeError(\"Incompatible shapes for dot: got {} and {}.\".format(\n\u001b[0;32m--> 634\u001b[0;31m         lhs.shape, rhs.shape))\n\u001b[0m\u001b[1;32m    635\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Incompatible shapes for dot: got (2,) and (3, 2)."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMzBunJZpFdt"
      },
      "source": [
        "We can manually vectorize the code by remembering the shapes, so \n",
        "$X w$ multiplies each row of $X$ with $w$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xD11HZSpotEM",
        "outputId": "99378a83-b75e-4b0b-9131-6357742658de"
      },
      "source": [
        "def predict_batch(X):\n",
        "    return sigmoid(jnp.dot(X, w)) # (N,D) * (D,1) = (N,1) # matrix-vector multiply\n",
        "\n",
        "print(predict_batch(X)) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.223 0.636 0.427]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZSksj-1GZcU"
      },
      "source": [
        "Fortunately we can use vmap."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "suWNVZ5uphgc",
        "outputId": "0914178e-fbe2-42ef-dc35-3dd20d4e61e0"
      },
      "source": [
        "print(vmap(predict_single)(X))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.223 0.636 0.427]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vJjjuKV9D-y"
      },
      "source": [
        "## Failure cases\n",
        "\n",
        "Vmap requires that the shapes of all the variables that are created by the function that is being mapped are the same for all values of the input arguments, as explained [here](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html). So vmap cannot be used to do any kind of embarassingly parallel task. Below we give a simple example of where this fails, since internally we create a vector whose length depends on the input 'length'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nz_YLnDV9VlC",
        "outputId": "6651abc2-1462-4bbd-86b0-ba0480f9c35a"
      },
      "source": [
        "def example_fun(length, val=4):\n",
        "  return jnp.sum(jnp.ones((length,)) * val)\n",
        "\n",
        "xs = jnp.arange(1,10)\n",
        "\n",
        "# Python map works fine\n",
        "v = list(map(example_fun, xs))\n",
        "print(v)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[DeviceArray(4., dtype=float32), DeviceArray(8., dtype=float32), DeviceArray(12., dtype=float32), DeviceArray(16., dtype=float32), DeviceArray(20., dtype=float32), DeviceArray(24., dtype=float32), DeviceArray(28., dtype=float32), DeviceArray(32., dtype=float32), DeviceArray(36., dtype=float32)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XM-v2F199jl7"
      },
      "source": [
        "The following fails."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "Bvq2JSsT9lh0",
        "outputId": "63e67f39-656a-4df2-86ce-5dc6a29cc564"
      },
      "source": [
        "v = vmap(example_fun)(xs)\n",
        "print(v)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-7fe93a3635da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample_fun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/api.py\u001b[0m in \u001b[0;36mbatched_fun\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1223\u001b[0m         \u001b[0mflat_fun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_axes_flat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m         \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mflatten_axes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"vmap out_axes\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_axes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1225\u001b[0;31m     ).call_wrapped(*args_flat)\n\u001b[0m\u001b[1;32m   1226\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtree_unflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_flat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/linear_util.py\u001b[0m in \u001b[0;36mcall_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m       \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m       \u001b[0;31m# Some transformations yield from inside context managers, so we have to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-4138480dd486>\u001b[0m in \u001b[0;36mexample_fun\u001b[0;34m(length, val)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mexample_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36mones\u001b[0;34m(shape, dtype)\u001b[0m\n\u001b[1;32m   2882\u001b[0m   \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat_\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2883\u001b[0m   \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2884\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2885\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/_src/lax/lax.py\u001b[0m in \u001b[0;36mfull\u001b[0;34m(shape, fill_value, dtype)\u001b[0m\n\u001b[1;32m   1448\u001b[0m       \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mcast\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1449\u001b[0m   \"\"\"\n\u001b[0;32m-> 1450\u001b[0;31m   \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcanonicalize_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1451\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1452\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"full must be called with scalar fill_value, got fill_value.shape {}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mcanonicalize_shape\u001b[0;34m(shape)\u001b[0m\n\u001b[1;32m   1202\u001b[0m     msg += (\"\\nIf using `jit`, try using `static_argnums` or applying `jit` to \"\n\u001b[1;32m   1203\u001b[0m             \"smaller subfunctions.\")\n\u001b[0;32m-> 1204\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Shapes must be 1D sequences of concrete values of integer type, got (Traced<ShapedArray(int32[])>with<BatchTrace(level=1/0)>\n  with val = DeviceArray([1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int32)\n       batch_dim = 0,).\nIf using `jit`, try using `static_argnums` or applying `jit` to smaller subfunctions."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X30m2EagpHSS"
      },
      "source": [
        "# Stochastics\n",
        "\n",
        "JAX is designed to be deterministic, but in some cases, we want to introduce randomness in a controlled way, and to reason about it. We discuss this below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUOZdeYBKjWc"
      },
      "source": [
        "## Random number generation\n",
        "\n",
        "One of the biggest differences from NumPy is the way Jax treates pseudo random number generation (PRNG).\n",
        "This is because Jax does not maintain any global state, i.e., it is purely functional.\n",
        "This design \"provides reproducible results invariant to compilation boundaries and backends,\n",
        "while also maximizing performance by enabling vectorized generation and parallelization across random calls\"\n",
        "(to quote [the official page](https://github.com/google/jax#a-brief-tour)).\n",
        "\n",
        "For example, consider this Numpy snippet. Each call to np.random.uniform updates the global state. The value of foo() is therefore only guaranteed to give the same result every time if we evaluate bar() and baz() in the same order (eg left to right). This is why foo1 and foo2 give different answers.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-uNUHTyuamz",
        "outputId": "a9d6beec-d732-47c0-8f26-fc4452ff3ab9"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def bar(): return np.random.uniform(size=(3))\n",
        "def baz(): return np.random.uniform(size=(3))\n",
        "\n",
        "def foo(seed): \n",
        "  np.random.seed(seed)\n",
        "  return bar() + 2*baz()\n",
        "\n",
        "def foo1(seed): \n",
        "  np.random.seed(seed)\n",
        "  a = bar()\n",
        "  b =  2*baz()\n",
        "  return a+b\n",
        "\n",
        "def foo2(seed): \n",
        "  np.random.seed(seed)\n",
        "  a = 2*baz() \n",
        "  b = bar()\n",
        "  return a+b\n",
        "\n",
        "seed = 0\n",
        "\n",
        "print(foo(seed))\n",
        "print(foo1(seed))\n",
        "print(foo2(seed))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.639 1.562 1.895]\n",
            "[1.639 1.562 1.895]\n",
            "[1.643 1.854 1.851]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5GLmTGJuqG_"
      },
      "source": [
        "\n",
        "\n",
        "Jax may evaluate parts of expressions such as `bar() + baz()` in parallel, which would violate reproducibility. To prevent this, the user must pass in an explicit PRNG key to every function that requires a source of randomness. Using the same key will give the same results.See the example below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcTYfznjKeHC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67cce2d2-2db2-4a72-93a5-d3f83c85cadb"
      },
      "source": [
        "\n",
        "\n",
        "key = random.PRNGKey(0)\n",
        "print(random.normal(key, shape=(3,)))  # [ 1.81608593 -0.48262325  0.33988902]\n",
        "print(random.normal(key, shape=(3,)))  # [ 1.81608593 -0.48262325  0.33988902]  ## identical results\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 1.816 -0.483  0.34 ]\n",
            "[ 1.816 -0.483  0.34 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfARZPjrvd_z"
      },
      "source": [
        "When generating independent samples, it is important to use different keys, to ensure results are not correlated. We can do this by *splitting* the key into the the 'master' key (which will be used in later parts of the code via splitting), and the 'subkey', which is used temporarily to generate randomness and then thrown away, as we illustrate below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUOY3pkpvnUN",
        "outputId": "4cd7f50a-6f47-4945-b184-3ef7b7976263"
      },
      "source": [
        "# To make a new key, we split the current key into two pieces.\n",
        "key, subkey = random.split(key)\n",
        "print(random.normal(subkey, shape=(3,)))  # [ 1.1378783  -1.22095478 -0.59153646]\n",
        "\n",
        "# We can continue to split off new pieces from the global key.\n",
        "key, subkey = random.split(key)\n",
        "print(random.normal(subkey, shape=(3,)))  # [-0.06607265  0.16676566  1.17800343]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 1.138 -1.221 -0.592]\n",
            "[-0.066  0.167  1.178]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oETNlM1RzWXK"
      },
      "source": [
        "We now reimplement the numpy example in Jax and show that we get the result no matter the order of evaluation of bar and baz."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTasDPbRwDrk",
        "outputId": "6e84846b-3304-48f6-8a81-d7265296c0ac"
      },
      "source": [
        "\n",
        "def bar(key): \n",
        "  return jax.random.uniform(key,shape=(3,))\n",
        "\n",
        "def baz(key):\n",
        "  return jax.random.uniform(key,shape=(3,))\n",
        "\n",
        "def foo(key): \n",
        "  subkey1, subkey2 = random.split(key, num=2) \n",
        "  return bar(subkey1) + 2 * baz(subkey2)\n",
        "\n",
        "def foo1(key): \n",
        "  subkey1, subkey2 = random.split(key, num=2) \n",
        "  a = bar(subkey1) \n",
        "  b =  2 * baz(subkey2)\n",
        "  return a+b\n",
        "\n",
        "def foo2(key): \n",
        "  subkey1, subkey2 = random.split(key, num=2) \n",
        "  a = 2 * baz(subkey2)\n",
        "  b = bar(subkey1)\n",
        "  return a+b\n",
        "\n",
        "key = random.PRNGKey(0)\n",
        "key, subkey = random.split(key) \n",
        "print(foo(subkey))\n",
        "print(foo1(subkey))\n",
        "print(foo2(subkey))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2.079 2.002 1.089]\n",
            "[2.079 2.002 1.089]\n",
            "[2.079 2.002 1.089]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAlQEKHOzfUG"
      },
      "source": [
        "In Jax (but not in python),  a random draw of N samples in parallel will not give the same results as N draws of individual samples, as we show below. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLRJkXRvz3YL",
        "outputId": "b821fb71-895a-4dc4-a81a-9ff748567221"
      },
      "source": [
        "key = random.PRNGKey(42)\n",
        "subkeys = random.split(key, 3)\n",
        "sequence = np.stack([jax.random.normal(subkey) for subkey in subkeys])\n",
        "print(\"individually:\", sequence)\n",
        "\n",
        "key = random.PRNGKey(42)\n",
        "print(\"all at once: \", jax.random.normal(key, shape=(3,)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "individually: [-0.048  0.108 -1.223]\n",
            "all at once:  [ 0.187 -1.281 -1.559]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wr1Kw7vyz5u-",
        "outputId": "7ba33e6c-cd01-4c9b-e10c-1cdf8e543ca0"
      },
      "source": [
        "np.random.seed(0)\n",
        "sequence = np.stack([np.random.normal() for i in range(3)])\n",
        "print(\"individually:\", sequence)\n",
        "\n",
        "np.random.seed(0)\n",
        "print(\"all at once: \", np.random.normal(size=(3,)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "individually: [1.764 0.4   0.979]\n",
            "all at once:  [1.764 0.4   0.979]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BrxgzFebytZ"
      },
      "source": [
        "Haiku has a handy method for generating a sequence of random keys from a seed key, as we show below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMIRGLKYb4N9",
        "outputId": "3f7d5d97-3c25-4d58-edab-111ca6e0cc86"
      },
      "source": [
        "import haiku as hk\n",
        "rng = jax.random.PRNGKey(0)\n",
        "rng_seq = hk.PRNGSequence(rng)\n",
        "for i in range(5):\n",
        "  rng = next(rng_seq)\n",
        "  samples = jax.random.bernoulli(rng, 0.5, shape=(3,))\n",
        "  print(samples)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[False  True  True]\n",
            "[ True False False]\n",
            "[ True  True  True]\n",
            "[False False  True]\n",
            "[ True  True False]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvFlhTJUpSZJ"
      },
      "source": [
        "## Probability distributions\n",
        "\n",
        "*TODO*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qA2RYhnNG9hh"
      },
      "source": [
        "# Autograd <a class=\"anchor\" id=\"AD\"></a>\n",
        "\n",
        "In this section, we illustrate automatic differentation using JAX.\n",
        "For details, see see  [this video](https://www.youtube.com/watch?v=wG_nF1awSSY&t=697s)  or [The Autodiff Cookbook](https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuMHSd3wr1xH"
      },
      "source": [
        "## Derivatives\n",
        "\n",
        "We can compute $(\\nabla f)(x)$ using `grad(f)(x)`. For example, consider\n",
        "\n",
        "\n",
        "$f(x) = x^3 + 2x^2 - 3x + 1$\n",
        "\n",
        "$f'(x) = 3x^2 + 4x -3$\n",
        "\n",
        "$f''(x) = 6x + 4$\n",
        "\n",
        "$f'''(x) = 6$\n",
        "\n",
        "$f^{iv}(x) = 0$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYTM5MPmr3C0",
        "outputId": "4801ef39-5c62-4fd1-c021-dc2d23e03035"
      },
      "source": [
        "f = lambda x: x**3 + 2*x**2 - 3*x + 1\n",
        "\n",
        "dfdx = jax.grad(f)\n",
        "d2fdx = jax.grad(dfdx)\n",
        "d3fdx = jax.grad(d2fdx)\n",
        "d4fdx = jax.grad(d3fdx)\n",
        "\n",
        "print(dfdx(1.))\n",
        "print(d2fdx(1.))\n",
        "print(d3fdx(1.))\n",
        "print(d4fdx(1.))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4.0\n",
            "10.0\n",
            "6.0\n",
            "0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUj-VuSzmFaV"
      },
      "source": [
        "## Partial derivatives\n",
        "\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "f(x,y) &= x^2 + y \\\\\n",
        "\\frac{\\partial f}{\\partial x} &= 2x \\\\\n",
        "\\frac{\\partial f}{\\partial y} &= 1 \n",
        "\\end{align}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0hW7fqfmR1c",
        "outputId": "3b5bbc43-a7e3-4692-c4e8-137faa0c68eb"
      },
      "source": [
        "def f(x,y):\n",
        "  return x**2 + y\n",
        "\n",
        "# Partial derviatives\n",
        "x = 2.0; y= 3.0;\n",
        "v, gx = value_and_grad(f, argnums=0)(x,y)\n",
        "print(v)\n",
        "print(gx)\n",
        "\n",
        "gy = grad(f, argnums=1)(x,y)\n",
        "print(gy)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7.0\n",
            "4.0\n",
            "1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTIybB8b4ar0"
      },
      "source": [
        "## Gradients "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xb0gZ_1HBEyC"
      },
      "source": [
        "Linear function: multi-input, scalar output.\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "f(x; a) &= a^T x\\\\\n",
        "\\nabla_x f(x;a) &= a\n",
        "\\end{align}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmYqFEs04vkV",
        "outputId": "311e8ac4-3ed4-4ad6-f545-6307390d4745"
      },
      "source": [
        "\n",
        "\n",
        "def fun1d(x):\n",
        "    return jnp.dot(a, x)[0]\n",
        "\n",
        "Din = 3; Dout = 1;\n",
        "a = np.random.normal(size=(Dout, Din))\n",
        "x = np.random.normal(size=(Din,))\n",
        "\n",
        "g = grad(fun1d)(x)\n",
        "assert np.allclose(g, a)\n",
        "\n",
        "\n",
        "# It is often useful to get the function value and gradient at the same time\n",
        "val_grad_fn = jax.value_and_grad(fun1d)\n",
        "v, g = val_grad_fn(x)\n",
        "print(v)\n",
        "print(g)\n",
        "assert np.allclose(v, fun1d(x))\n",
        "assert np.allclose(a, g)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-1.0599848\n",
            "[-1.311  0.546  0.915]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbgiqkF6BL1E"
      },
      "source": [
        "Linear function: multi-input, multi-output.\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "f(x;A) &= A x \\\\\n",
        "\\frac{\\partial f(x;A)}{\\partial x} &= A\n",
        "\\end{align}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6hkEYxV5EIx"
      },
      "source": [
        "# We construct a multi-output linear function.\n",
        "# We check forward and reverse mode give same Jacobians.\n",
        "\n",
        "\n",
        "def fun(x):\n",
        "    return jnp.dot(A, x)\n",
        "\n",
        "Din = 3; Dout = 4;\n",
        "A = np.random.normal(size=(Dout, Din))\n",
        "x = np.random.normal(size=(Din,))\n",
        "Jf = jacfwd(fun)(x)\n",
        "Jr = jacrev(fun)(x)\n",
        "assert np.allclose(Jf, Jr)\n",
        "assert np.allclose(Jf, A)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CN5d-D7XBU9Y"
      },
      "source": [
        "Quadratic form.\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "f(x;A) &= x^T A x \\\\\n",
        "\\nabla_x f(x;A) &= (A+A^T) x\n",
        "\\end{align}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9URZeX8PBbhl"
      },
      "source": [
        "\n",
        "D = 4\n",
        "A = np.random.normal(size=(D,D))\n",
        "x = np.random.normal(size=(D,))\n",
        "quadfun = lambda x: jnp.dot(x, jnp.dot(A, x))\n",
        "\n",
        "g = grad(quadfun)(x)\n",
        "assert np.allclose(g, jnp.dot(A+A.T, x))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9ZOhDeqCXu3"
      },
      "source": [
        "Chain rule applied to sigmoid function.\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\mu(x;w) &=\\sigma(w^T x) \\\\\n",
        "\\nabla_w \\mu(x;w) &= \\sigma'(w^T x) x \\\\\n",
        "\\sigma'(a) &= \\sigma(a) * (1-\\sigma(a)) \n",
        "\\end{align}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6q5VfLXLB7rv",
        "outputId": "0773a46f-784f-4dbe-a6b7-29dd5cd26654"
      },
      "source": [
        "\n",
        "\n",
        "D = 4\n",
        "w = np.random.normal(size=(D,))\n",
        "x = np.random.normal(size=(D,))\n",
        "y = 0 \n",
        "\n",
        "def sigmoid(x): return 0.5 * (jnp.tanh(x / 2.) + 1)\n",
        "def mu(w): return sigmoid(jnp.dot(w,x))\n",
        "def deriv_mu(w): return mu(w) * (1-mu(w)) * x\n",
        "deriv_mu_jax =  grad(mu)\n",
        "\n",
        "print(deriv_mu(w))\n",
        "print(deriv_mu_jax(w))\n",
        "\n",
        "assert np.allclose(deriv_mu(w), deriv_mu_jax(w), atol=1e-3)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.458  0.022 -0.266 -0.005]\n",
            "[-0.458  0.022 -0.266 -0.005]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nglie5m7q607"
      },
      "source": [
        "## Auxiliary return values\n",
        "\n",
        "A function can return its value and other auxiliary results; the latter are not differentiated. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHz6zrC9qVjT",
        "outputId": "76724fd1-4e1a-4737-b252-963700fcce91"
      },
      "source": [
        "def f(x,y):\n",
        "  return x**2+y, 42\n",
        "\n",
        "(v,aux), g = value_and_grad(f, has_aux=True)(x,y)\n",
        "print(v)\n",
        "print(aux)\n",
        "print(g)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7.0\n",
            "42\n",
            "4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBMcsg4uoKua"
      },
      "source": [
        "## Jacobians\n",
        "\n",
        "\n",
        "Example: Linear function: multi-input, multi-output.\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "f(x;A) &= A x \\\\\n",
        "\\frac{\\partial f(x;A)}{\\partial x} &= A\n",
        "\\end{align}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPilm5H3oWcy"
      },
      "source": [
        "# We construct a multi-output linear function.\n",
        "# We check forward and reverse mode give same Jacobians.\n",
        "\n",
        "\n",
        "def fun(x):\n",
        "    return jnp.dot(A, x)\n",
        "\n",
        "Din = 3; Dout = 4;\n",
        "A = np.random.normal(size=(Dout, Din))\n",
        "x = np.random.normal(size=(Din,))\n",
        "Jf = jacfwd(fun)(x)\n",
        "Jr = jacrev(fun)(x)\n",
        "assert np.allclose(Jf, Jr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mg9ValMRm_Md"
      },
      "source": [
        "## Hessians\n",
        "\n",
        "Quadratic form.\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "f(x;A) &= x^T A x \\\\\n",
        "\\nabla_x^2 f(x;A) &= A + A^T\n",
        "\\end{align}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leW9lqvinDsM"
      },
      "source": [
        "\n",
        "D = 4\n",
        "A = np.random.normal(size=(D,D))\n",
        "x = np.random.normal(size=(D,))\n",
        "\n",
        "quadfun = lambda x: jnp.dot(x, jnp.dot(A, x))\n",
        "\n",
        "\n",
        "H1 = hessian(quadfun)(x)\n",
        "assert np.allclose(H1, A+A.T)\n",
        "\n",
        "def my_hessian(fun):\n",
        "  return jacfwd(jacrev(fun))\n",
        "\n",
        "H2 = my_hessian(quadfun)(x)\n",
        "assert np.allclose(H1, H2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeoGcnV54YY9"
      },
      "source": [
        "## Example: Binary logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Isql2l4MGfIt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2465d977-66dc-4ea0-e5bd-5a05d4ad92ec"
      },
      "source": [
        "\n",
        "def sigmoid(x): return 0.5 * (jnp.tanh(x / 2.) + 1)\n",
        "\n",
        "def predict_single(w, x):\n",
        "    return sigmoid(jnp.dot(w, x)) # <(D) , (D)> = (1) # inner product\n",
        "  \n",
        "def predict_batch(w, X):\n",
        "    return sigmoid(jnp.dot(X, w)) # (N,D) * (D,1) = (N,1) # matrix-vector multiply\n",
        "\n",
        "# negative log likelihood\n",
        "def loss(weights, inputs, targets):\n",
        "    preds = predict_batch(weights, inputs)\n",
        "    logprobs = jnp.log(preds) * targets + jnp.log(1 - preds) * (1 - targets)\n",
        "    return -jnp.sum(logprobs)\n",
        "\n",
        "\n",
        "D = 2\n",
        "N = 3\n",
        "w = jax.random.normal(key, shape=(D,))\n",
        "X = jax.random.normal(key, shape=(N,D))\n",
        "y = jax.random.choice(key, 2, shape=(N,)) # uniform binary labels\n",
        "#logits = jnp.dot(X, w)\n",
        "#y = jax.random.categorical(key, logits)\n",
        "\n",
        "print(loss(w, X, y))\n",
        "\n",
        "# Gradient function\n",
        "grad_fun = grad(loss)\n",
        "\n",
        "# Gradient of each example in the batch - 2 different ways\n",
        "grad_fun_w = partial(grad_fun, w)\n",
        "grads = vmap(grad_fun_w)(X,y)\n",
        "print(grads)\n",
        "assert grads.shape == (N,D)\n",
        "\n",
        "grads2 = vmap(grad_fun, in_axes=(None, 0, 0))(w, X, y) \n",
        "assert np.allclose(grads, grads2)\n",
        "\n",
        "# Gradient for entire batch\n",
        "grad_sum = jnp.sum(grads, axis=0)\n",
        "assert grad_sum.shape == (D,)\n",
        "print(grad_sum)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.5545294\n",
            "[[ 0.042 -0.287]\n",
            " [-0.236 -0.454]\n",
            " [-0.14   0.067]]\n",
            "[-0.334 -0.673]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3BaHdT4Gj6W",
        "outputId": "12dc846e-dd7e-4907-ea76-73147ea6f77f"
      },
      "source": [
        "# Textbook implementation of gradient\n",
        "def NLL_grad(weights, batch):\n",
        "    X, y = batch\n",
        "    N = X.shape[0]\n",
        "    mu = predict_batch(weights, X)\n",
        "    g = jnp.sum(jnp.dot(jnp.diag(mu - y), X), axis=0)\n",
        "    return g\n",
        "\n",
        "grad_sum_batch = NLL_grad(w, (X,y))\n",
        "print(grad_sum_batch)\n",
        "assert np.allclose(grad_sum, grad_sum_batch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.334 -0.673]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_4lRrHgpLbG",
        "outputId": "d47db27a-b171-4933-de08-58bc5bbe0c2f"
      },
      "source": [
        "# We can also compute Hessians, as we illustrate below.\n",
        "\n",
        "hessian_fun = hessian(loss)\n",
        "\n",
        "# Hessian on one example\n",
        "H0 = hessian_fun(w, X[0,:], y[0])\n",
        "print('Hessian(example 0)\\n{}'.format(H0))\n",
        "\n",
        "# Hessian for batch\n",
        "Hbatch = vmap(hessian_fun, in_axes=(None, 0, 0))(w, X, y) \n",
        "print('Hbatch shape {}'.format(Hbatch.shape))\n",
        "\n",
        "Hbatch_sum = jnp.sum(Hbatch, axis=0)\n",
        "print('Hbatch sum\\n {}'.format(Hbatch_sum))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hessian(example 0)\n",
            "[[ 0.006 -0.042]\n",
            " [-0.042  0.286]]\n",
            "Hbatch shape (3, 2, 2)\n",
            "Hbatch sum\n",
            " [[0.118 0.139]\n",
            " [0.139 0.65 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcJvgukUpWWE"
      },
      "source": [
        "# Textbook implementation of Hessian\n",
        "\n",
        "def NLL_hessian(weights, batch):\n",
        "  X, y = batch\n",
        "  mu = predict_batch(weights, X)\n",
        "  S = jnp.diag(mu * (1-mu))\n",
        "  H = jnp.dot(jnp.dot(X.T, S), X)\n",
        "  return H\n",
        "\n",
        "H2 = NLL_hessian(w, (X,y) )\n",
        "\n",
        "assert np.allclose(Hbatch_sum, H2, atol=1e-2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOzQJSX3JOF9"
      },
      "source": [
        "## Vector Jacobian Products (VJP) and Jacobian Vector Products (JVP)\n",
        "\n",
        "Consider a bilinear mapping $f(x,W) = x W$.\n",
        "For fixed parameters, we have\n",
        "$f1(x) = W x$, so $J(x) = W$, and $u^T J(x) = J(x)^T u = W^T u$.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Lpmntn2JREG",
        "outputId": "39778872-aca9-488c-f9dc-40d30a821488"
      },
      "source": [
        "n = 3; m = 2;\n",
        "W = jax.random.normal(key, shape=(m,n))\n",
        "x = jax.random.normal(key, shape=(n,))\n",
        "u = jax.random.normal(key, shape=(m,))\n",
        "\n",
        "def f1(x): return jnp.dot(W,x)\n",
        "\n",
        "J1 = jacfwd(f1)(x)\n",
        "print(J1.shape)\n",
        "\n",
        "assert np.allclose(J1, W)\n",
        "tmp1 = jnp.dot(u.T, J1)\n",
        "print(tmp1)\n",
        "\n",
        "(val, jvp_fun) = jax.vjp(f1, x)\n",
        "\n",
        "tmp2 = jvp_fun(u)\n",
        "\n",
        "assert np.allclose(tmp1, tmp2)\n",
        "\n",
        "tmp3 = np.dot(W.T, u)\n",
        "assert np.allclose(tmp1, tmp3)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2, 3)\n",
            "[ 0.922  1.216 -0.61 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYSC6DMOO3IS"
      },
      "source": [
        "For fixed inputs, we have\n",
        "$f2(W) = W x$, so $J(W) = \\text{something complex}$,\n",
        "but $u^T J(W) = J(W)^T u = u x^T$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8l3StJdO1r1",
        "outputId": "5a3f1e5b-b903-4db8-bb8a-a53c29bcad19"
      },
      "source": [
        "\n",
        "def f2(W): return jnp.dot(W,x)\n",
        "\n",
        "J2 = jacfwd(f2)(W)\n",
        "print(J2.shape)\n",
        "\n",
        "tmp1 = jnp.dot(u.T, J2)\n",
        "print(tmp1)\n",
        "print(tmp1.shape)\n",
        "\n",
        "(val, jvp_fun) = jax.vjp(f2, W)\n",
        "tmp2 = jvp_fun(u)\n",
        "assert np.allclose(tmp1, tmp2)\n",
        "\n",
        "tmp3 = np.outer(u, x)\n",
        "assert np.allclose(tmp1, tmp3)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2, 2, 3)\n",
            "[[-1.425  0.379 -0.267]\n",
            " [ 1.555 -0.413  0.291]]\n",
            "(2, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZF_OjIRrisDB"
      },
      "source": [
        "## Stop-gradient\n",
        "\n",
        "Sometimes we want to take the gradient of a complex expression wrt some parameters $\\theta$, but treating $\\theta$ as a constant for some parts of the expression. For example, consider the TD(0) update in reinforcement learning, which as the following form:\n",
        "\n",
        "\n",
        "$\\Delta \\theta = (r_t + v_{\\theta}(s_t) - v_{\\theta}(s_{t-1})) \\nabla v_{\\theta}(s_{t-1})$\n",
        "\n",
        "where $s$ is the state, $r$ is the reward, and $v$ is the value function.\n",
        "This update is not the gradient of any loss function.\n",
        "However it can be **written** as the gradient of the pseudo loss function\n",
        "\n",
        "$L(\\theta) = [r_t + v_{\\theta}(s_t) - v_{\\theta}(s_{t-1})]^2$\n",
        "\n",
        "since\n",
        "\n",
        "$\\nabla_{\\theta} L(\\theta) = 2 [r_t + v_{\\theta}(s_t) - v_{\\theta}(s_{t-1})] \\nabla v_{\\theta}(s_{t-1})$\n",
        "\n",
        "if the dependency of the target $r_t + v_{\\theta}(s_t)$ on the parameter $\\theta$ is ignored. We can implement this in JAX using `stop_gradient`, as we show below.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9cprxb9jbsK",
        "outputId": "b345e287-b8fa-417f-d20e-0f1951e1f64e"
      },
      "source": [
        "def td_loss(theta, s_prev, r_t, s_t):\n",
        "  v_prev = value_fn(theta, s_prev)\n",
        "  target = r_t + value_fn(theta, s_t)\n",
        "  return 0.5*(jax.lax.stop_gradient(target) - v_prev) ** 2\n",
        "\n",
        "td_update = jax.grad(td_loss)\n",
        "\n",
        "# An example transition.\n",
        "s_prev = jnp.array([1., 2., -1.])\n",
        "r_t = jnp.array(1.)\n",
        "s_t = jnp.array([2., 1., 0.])\n",
        "\n",
        "# Value function and initial parameters\n",
        "value_fn = lambda theta, state: jnp.dot(theta, state)\n",
        "theta = jnp.array([0.1, -0.1, 0.])\n",
        "\n",
        "print(td_update(theta, s_prev, r_t, s_t))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-1.2 -2.4  1.2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKTg9m9yz7Ib"
      },
      "source": [
        "## Straight through estimator\n",
        "\n",
        "The straight-through estimator is a trick for defining a 'gradient' of a function that is otherwise non-differentiable. Given a non-differentiable function $f : \\mathbb{R}^n \\to \\mathbb{R}^n$ that is used as part of a larger function that we wish to find a gradient of, we simply pretend during the backward pass that $f$ is the identity function, so gradients pass through $f$ ignoring the $f'$ term. This can be implemented neatly using `jax.lax.stop_gradient`.\n",
        "\n",
        "Here is an example of a non-differentiable function that converts a soft probability distribution to a one-hot vector (discretization).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIFVQKrwqAG4",
        "outputId": "67ada739-47e3-4ef9-c11e-76a64862ea80"
      },
      "source": [
        "def onehot(labels, num_classes):\n",
        "  y = (labels[..., None] == jnp.arange(num_classes)[None])\n",
        "  return y.astype(jnp.float32)\n",
        "\n",
        "def quantize(y_soft): \n",
        "  y_hard = onehot(jnp.argmax(y_soft), 3)[0]\n",
        "  return y_hard\n",
        "\n",
        "y_soft = np.array([0.1, 0.2, 0.7])\n",
        "print(quantize(y_soft))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSuQMr61sg16"
      },
      "source": [
        "Now suppose we define some linear function of the quantized variable of the form $f(y) = w^T q(y)$. If $w=[1,2,3]$ and $q(y)=[0,0,1]$, we get $f(y) = 3$. But the gradient is 0 because $q$ is not differentiable.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDEMPSdJsTpl",
        "outputId": "0f329a87-2271-47d2-f23b-be47ed79a618"
      },
      "source": [
        "def f(y):\n",
        "  w = jnp.array([1,2,3])\n",
        "  yq = quantize(y)\n",
        "  return jnp.dot(w, yq)\n",
        "\n",
        "print(f(y_soft))\n",
        "print(grad(f)(y_soft))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3.0\n",
            "[0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTcVC08Rs4DM"
      },
      "source": [
        "To use the straight-through estimator, we replace $q(y)$ with \n",
        "$$y + SG(q(y)-y)$$, where SG is stop gradient. In the forwards pass, we have $y+q(y)-y=q(y)$. In the backwards pass, the gradient of SG is 0, so we effectively replace $q(y)$ with $y$. So in the backwarsd pass we have\n",
        "$$\n",
        "\\begin{align}\n",
        "f(y) &= w^T q(y) \\approx w^T  y \\\\\n",
        "\\nabla_y f(y) &\\approx w\n",
        "\\end{align}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5m_k7Ju3sUVq",
        "outputId": "cfcd7769-de40-4fc7-d29c-19d6a7d04db0"
      },
      "source": [
        "\n",
        "\n",
        "def f_ste(y):\n",
        "  w = jnp.array([1,2,3])\n",
        "  yq = quantize(y)\n",
        "  yy = y + jax.lax.stop_gradient(yq - y) # gives yq on fwd, and y on backward\n",
        "  return jnp.dot(w, yy)\n",
        "\n",
        "print(f_ste(y_soft))\n",
        "print(grad(f_ste)(y_soft))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3.0\n",
            "[1. 2. 3.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwMIYqiOp0U0"
      },
      "source": [
        "## Per-example gradients\n",
        "\n",
        "In some applications, we want to compute the gradient for every example in a batch, not just the sum of gradients over the batch. This is hard in other frameworks like TF and PyTorch but easy in JAX, as we show below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKhNPiq4qGhl",
        "outputId": "0102b6ff-1fc1-4870-9f58-1b4e7db92ba9"
      },
      "source": [
        "def loss(w, x):\n",
        "  return jnp.dot(w,x)\n",
        "\n",
        "w = jnp.ones((3,))\n",
        "x0 = jnp.array([1.0, 2.0, 3.0])\n",
        "x1 = 2*x0\n",
        "X = jnp.stack([x0, x1])\n",
        "print(X.shape)\n",
        "\n",
        "perex_grads = jax.jit(jax.vmap(jax.grad(loss), in_axes=(None, 0)))\n",
        "print(perex_grads(w, X))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2, 3)\n",
            "[[1. 2. 3.]\n",
            " [2. 4. 6.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxPeXpUaq-_p"
      },
      "source": [
        "To explain the above code in more depth, note that the vmap converts the function loss to take  a batch of inputs for each of its arguments, and returns a batch of outputs. To make it work with a single weight vector, we specify in_axes=(None,0), meaning the first argument (w) is not replicated, and the second argument (x) is replicated along dimension 0. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYB1L8JKrVKo",
        "outputId": "b78026f2-7ff2-4228-be72-cd0c1b281b2b"
      },
      "source": [
        "gradfn = jax.grad(loss)\n",
        "\n",
        "W = jnp.stack([w, w])\n",
        "print(jax.vmap(gradfn)(W, X))\n",
        "\n",
        "print(jax.vmap(gradfn, in_axes=(None,0))(w, X))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1. 2. 3.]\n",
            " [2. 4. 6.]]\n",
            "[[1. 2. 3.]\n",
            " [2. 4. 6.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnOIJRGxJigp"
      },
      "source": [
        "\n",
        "# JIT (just in time compilation) <a class=\"anchor\" id=\"JIT\"></a>\n",
        "\n",
        "In this section, we illustrate how to use the Jax JIT compiler to make code go faster (even on a CPU). However, it does not work on arbitrary Python code, as we explain below.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGZUbPDLKIkv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64c69f4f-5ddf-4ac5-9c54-99604c25857a"
      },
      "source": [
        "\n",
        "\n",
        "def slow_f(x):\n",
        "  # Element-wise ops see a large benefit from fusion\n",
        "  return x * x + x * 2.0\n",
        "\n",
        "x = jnp.ones((5000, 5000))\n",
        "%timeit  slow_f(x) \n",
        "\n",
        "fast_f = jit(slow_f)\n",
        "%timeit fast_f(x)  \n",
        " \n",
        "assert np.allclose(slow_f(x), fast_f(x))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The slowest run took 242.68 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "1 loop, best of 3: 1.42 ms per loop\n",
            "The slowest run took 149.09 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "1000 loops, best of 3: 506 µs per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77-33YadKNni"
      },
      "source": [
        "We can also add the `@jit` decorator in front of a function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5L-AEDhAb8t",
        "outputId": "61b9d57d-7183-475a-fca1-335ae78cc21b"
      },
      "source": [
        "@jit\n",
        "def faster_f(x):\n",
        "  return x * x + x * 2.0\n",
        "  \n",
        "%timeit faster_f(x)\n",
        "assert np.allclose(faster_f(x), fast_f(x))  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The slowest run took 35.82 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "1000 loops, best of 3: 507 µs per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMZVMEOPbLWt"
      },
      "source": [
        "## How it works: Jaxprs and tracing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvnubTBKbPoN"
      },
      "source": [
        "In this section, we briefly explain the mechanics behind JIT, which will help you understand when it does not work.\n",
        "\n",
        "First, consider this function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWK7gOSibbrY"
      },
      "source": [
        "\n",
        "def f(x):\n",
        "  y = jnp.ones((1,5)) * x\n",
        "  return y\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkIFd3_lbnaM"
      },
      "source": [
        "When a function is first executed (applied to an argument), it is converted to an intermediate representatio called a JAX expression or jaxpr, by a process called tracing, as we show below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqNGq8z4bxlI",
        "outputId": "0a74946e-5186-4d0d-a6b4-d709454bfaa0"
      },
      "source": [
        "print(f(3.0))\n",
        "print(jax.make_jaxpr(f)(3.0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[3. 3. 3. 3. 3.]]\n",
            "{ lambda  ; a.\n",
            "  let b = broadcast_in_dim[ broadcast_dimensions=(  )\n",
            "                            shape=(1, 5) ] 1.0\n",
            "      c = mul b a\n",
            "  in (c,) }\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkVzG8Q4cCn5"
      },
      "source": [
        "The XLA JIT compiler can then convert the jaxpr to code that runs fast on a CPU, GPU or TPU; the original python code is no longer needed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zd9H0391dHGB",
        "outputId": "5ad00a01-a007-4af3-e3cd-3b962c07e47f"
      },
      "source": [
        "f_jit = jit(f)\n",
        "print(f_jit(3.0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[3. 3. 3. 3. 3.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAR-o-r_dMvS"
      },
      "source": [
        "However, the jaxpr is created by tracing the function for a specific value. If different code is executed depending on the value of the input arguments, the resulting jaxpr will be different, so the function cannot be JITed, as we illustrate below. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 618
        },
        "id": "fhH3LaxEbfUh",
        "outputId": "dd298808-16e1-4d63-a535-f262b90b5b3b"
      },
      "source": [
        "\n",
        "def f(x):\n",
        "  if x > 0:\n",
        "    return x\n",
        "  else:\n",
        "    return 2 * x\n",
        "\n",
        "print(f(3.0))\n",
        "\n",
        "f_jit = jit(f)\n",
        "print(f_jit(3.0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ConcretizationTypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mConcretizationTypeError\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-a92ed15cd10b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mf_jit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_jit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/api.py\u001b[0m in \u001b[0;36mf_jitted\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcache_miss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# probably won't return\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mcpp_jitted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m   \u001b[0mf_jitted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cpp_jitted_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcpp_jitted_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/api.py\u001b[0m in \u001b[0;36mcache_miss\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0mbackend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_fun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         donated_invars=donated_invars)\n\u001b[0m\u001b[1;32m    285\u001b[0m     \u001b[0mout_pytree_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_unflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_pytree_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_flat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mbind\u001b[0;34m(self, fun, *args, **params)\u001b[0m\n\u001b[1;32m   1227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1229\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_bind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mcall_bind\u001b[0;34m(primitive, fun, *args, **params)\u001b[0m\n\u001b[1;32m   1218\u001b[0m   \u001b[0mtracers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_trace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_raise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mmaybe_new_sublevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_trace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1220\u001b[0;31m     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_trace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1221\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_lower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_todos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_trace_todo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, trace, fun, tracers, params)\u001b[0m\n\u001b[1;32m   1230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1232\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mpost_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_tracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mprocess_call\u001b[0;34m(self, primitive, f, tracers, params)\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    599\u001b[0m   \u001b[0mprocess_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/interpreters/xla.py\u001b[0m in \u001b[0;36m_xla_call_impl\u001b[0;34m(fun, device, backend, name, donated_invars, *args)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_xla_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWrappedFun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdonated_invars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m   compiled_fun = _xla_callable(fun, device, backend, name, donated_invars,\n\u001b[0;32m--> 570\u001b[0;31m                                *unsafe_map(arg_spec, args))\n\u001b[0m\u001b[1;32m    571\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcompiled_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/linear_util.py\u001b[0m in \u001b[0;36mmemoized_fun\u001b[0;34m(fun, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m       \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopulate_stores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m       \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m       \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/interpreters/xla.py\u001b[0m in \u001b[0;36m_xla_callable\u001b[0;34m(fun, device, backend, name, donated_invars, *arg_specs)\u001b[0m\n\u001b[1;32m    643\u001b[0m   \u001b[0mabstract_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_devices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munzip2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_specs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0momnistaging_enabled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 645\u001b[0;31m     \u001b[0mjaxpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_avals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_to_jaxpr_final\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabstract_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    646\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTracer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconsts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnexpectedTracerError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Encountered an unexpected tracer.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/interpreters/partial_eval.py\u001b[0m in \u001b[0;36mtrace_to_jaxpr_final\u001b[0;34m(fun, in_avals)\u001b[0m\n\u001b[1;32m   1228\u001b[0m     \u001b[0mmain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun_sourceinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1229\u001b[0m     \u001b[0mmain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjaxpr_stack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1230\u001b[0;31m     \u001b[0mjaxpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_avals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace_to_subjaxpr_dynamic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_avals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1231\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1232\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mjaxpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_avals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/interpreters/partial_eval.py\u001b[0m in \u001b[0;36mtrace_to_subjaxpr_dynamic\u001b[0;34m(fun, main, in_avals)\u001b[0m\n\u001b[1;32m   1209\u001b[0m     \u001b[0mtrace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDynamicJaxprTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcur_sublevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m     \u001b[0min_tracers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_arg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_avals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1211\u001b[0;31m     \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0min_tracers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1212\u001b[0m     \u001b[0mout_tracers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_raise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m   \u001b[0mjaxpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_avals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_jaxpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_tracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_tracers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/linear_util.py\u001b[0m in \u001b[0;36mcall_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m       \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m       \u001b[0;31m# Some transformations yield from inside context managers, so we have to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-a92ed15cd10b>\u001b[0m in \u001b[0;36mf\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/core.py\u001b[0m in \u001b[0;36m__bool__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    525\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m   \u001b[0;32mdef\u001b[0m \u001b[0m__bool__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    528\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__int__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__long__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_long\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/core.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, arg)\u001b[0m\n\u001b[1;32m    908\u001b[0m                       f\"or `jnp.array(x, {fun.__name__})` instead.\")\n\u001b[1;32m    909\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m     \u001b[0mraise_concretization_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mraise_concretization_error\u001b[0;34m(val, context)\u001b[0m\n\u001b[1;32m    897\u001b[0m          \u001b[0;34m\"See https://jax.readthedocs.io/en/latest/faq.html#abstract-tracer-value-encountered-where-concrete-value-is-expected-error for more information.\\n\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m           f\"Encountered tracer value: {val}\")\n\u001b[0;32m--> 899\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mConcretizationTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    900\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mConcretizationTypeError\u001b[0m: Abstract tracer value encountered where concrete value is expected.\n\nThe problem arose with the `bool` function. \n\nWhile tracing the function f at <ipython-input-13-a92ed15cd10b>:2, this concrete value was not available in Python because it depends on the value of the arguments to f at <ipython-input-13-a92ed15cd10b>:2 at flattened positions [0], and the computation of these values is being staged out (that is, delayed rather than executed eagerly).\n\nYou can use transformation parameters such as `static_argnums` for `jit` to avoid tracing particular arguments of transformed functions, though at the cost of more recompiles.\n\nSee https://jax.readthedocs.io/en/latest/faq.html#abstract-tracer-value-encountered-where-concrete-value-is-expected-error for more information.\n\nEncountered tracer value: Traced<ShapedArray(bool[])>with<DynamicJaxprTrace(level=0/1)>"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrOgfVbEePcw"
      },
      "source": [
        "Jit will create a new compiled version for each different ShapedArray, but will reuse the code for different values of the same shape. If the code path depends on the concrete value,  we can either just jit a subfunction (whose code path is constant), or we can create a different jaxpr for each concrete value of the input arguments as we explain below.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdtqCDWZAC2f"
      },
      "source": [
        "## Static argnum\n",
        "\n",
        "Note that JIT compilation requires that the control flow through the function  can be determined by the shape (but not concrete value) of its inputs. The function below violates this, since when x<0, it takes one branch, whereas when x>0, it takes the other."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ps1W8LhKKj9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e0d4759-4656-4000-d9cd-f51713a7145f"
      },
      "source": [
        "@jit\n",
        "def f(x):\n",
        "  if x > 0:\n",
        "    return x\n",
        "  else:\n",
        "    return 2 * x\n",
        "\n",
        "\n",
        "# This will fail!\n",
        "try:\n",
        "  print(f(3))\n",
        "except Exception as e:\n",
        "  print(\"ERROR:\", e)\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ERROR: Abstract tracer value encountered where concrete value is expected.\n",
            "\n",
            "The problem arose with the `bool` function. \n",
            "\n",
            "While tracing the function f at <ipython-input-17-94e6eda28128>:1, this concrete value was not available in Python because it depends on the value of the arguments to f at <ipython-input-17-94e6eda28128>:1 at flattened positions [0], and the computation of these values is being staged out (that is, delayed rather than executed eagerly).\n",
            "\n",
            "You can use transformation parameters such as `static_argnums` for `jit` to avoid tracing particular arguments of transformed functions, though at the cost of more recompiles.\n",
            "\n",
            "See https://jax.readthedocs.io/en/latest/faq.html#abstract-tracer-value-encountered-where-concrete-value-is-expected-error for more information.\n",
            "\n",
            "Encountered tracer value: Traced<ShapedArray(bool[])>with<DynamicJaxprTrace(level=0/1)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tHh4tcXKTRf"
      },
      "source": [
        "We can fix this by telling JAX to trace the control flow through the function using concrete values of some of its arguments. JAX will then compile different versions, depending on the input values. See below for an example.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMaRplccKRHc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38fd3892-6775-4394-8115-baddcc0ee4a8"
      },
      "source": [
        "\n",
        "\n",
        "def f(x):\n",
        "  if x > 0:\n",
        "    return x\n",
        "  else:\n",
        "    return 2 * x\n",
        "\n",
        "f = jit(f, static_argnums=(0,))\n",
        "\n",
        "print(f(3))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvcgAJEB9Dnn",
        "outputId": "3a58d3a6-f262-40d2-e5c4-02835be09390"
      },
      "source": [
        "@partial(jit, static_argnums=(0,))\n",
        "def f(x):\n",
        "  if x > 0:\n",
        "    return x\n",
        "  else:\n",
        "    return 2 * x\n",
        "print(f(3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snLung369jTw"
      },
      "source": [
        "## Jit and vmap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_Fkav2GRgnc"
      },
      "source": [
        "Unfortunately, the static argnum method fails when the function is passed to  vmap, because the latter can take arguments of different shape."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "QVLpm6n9faJp",
        "outputId": "a1234413-1408-4533-8e40-1b59539594a9"
      },
      "source": [
        "xs = jnp.arange(5)\n",
        "\n",
        "@partial(jit, static_argnums=(0,))\n",
        "def f(x):\n",
        "  if x > 0:\n",
        "    return x\n",
        "  else:\n",
        "    return 2 * x\n",
        "\n",
        "ys = vmap(f)(xs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-00960fe99363>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/api.py\u001b[0m in \u001b[0;36mbatched_fun\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   1187\u001b[0m                               lambda: flatten_axes(\"vmap out_axes\", out_tree(),\n\u001b[1;32m   1188\u001b[0m                                                    out_axes),\n\u001b[0;32m-> 1189\u001b[0;31m                               axis_name=axis_name)\n\u001b[0m\u001b[1;32m   1190\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtree_unflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_flat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/interpreters/batching.py\u001b[0m in \u001b[0;36mbatch\u001b[0;34m(fun, in_vals, in_dims, out_dim_dests, axis_name)\u001b[0m\n\u001b[1;32m     33\u001b[0m   \u001b[0;31m# executes a batched version of `fun` following out_dim_dests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m   \u001b[0mbatched_fun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_dim_dests\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mbatched_fun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0min_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mlu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformation_with_aux\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/linear_util.py\u001b[0m in \u001b[0;36mcall_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m       \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m       \u001b[0;31m# Some transformations yield from inside context managers, so we have to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/api.py\u001b[0m in \u001b[0;36mf_jitted\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcache_miss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# probably won't return\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mcpp_jitted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m   \u001b[0mf_jitted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cpp_jitted_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcpp_jitted_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Non-hashable static arguments are not supported. An error occured while trying to hash an object of type <class 'jax.interpreters.batching.BatchTracer'>, Traced<ShapedArray(int32[])>with<BatchTrace(level=1/0)>\n  with val = DeviceArray([0, 1, 2, 3, 4], dtype=int32)\n       batch_dim = 0. The error was:\nTypeError: unhashable type: 'BatchTracer'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "xbWYxD509Vze",
        "outputId": "d25118f8-fb3e-47f2-9434-b9accbc43d6a"
      },
      "source": [
        "def f(x):\n",
        "  if x > 0:\n",
        "    return x\n",
        "  else:\n",
        "    return 2 * x\n",
        "\n",
        "xs = jnp.arange(5)\n",
        "ys = jit(vmap(f)(xs))\n",
        "print(ys)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ConcretizationTypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mConcretizationTypeError\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-4c70fa606777>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/api.py\u001b[0m in \u001b[0;36mbatched_fun\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   1187\u001b[0m                               lambda: flatten_axes(\"vmap out_axes\", out_tree(),\n\u001b[1;32m   1188\u001b[0m                                                    out_axes),\n\u001b[0;32m-> 1189\u001b[0;31m                               axis_name=axis_name)\n\u001b[0m\u001b[1;32m   1190\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtree_unflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_flat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/interpreters/batching.py\u001b[0m in \u001b[0;36mbatch\u001b[0;34m(fun, in_vals, in_dims, out_dim_dests, axis_name)\u001b[0m\n\u001b[1;32m     33\u001b[0m   \u001b[0;31m# executes a batched version of `fun` following out_dim_dests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m   \u001b[0mbatched_fun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_dim_dests\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mbatched_fun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0min_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mlu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformation_with_aux\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/linear_util.py\u001b[0m in \u001b[0;36mcall_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m       \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m       \u001b[0;31m# Some transformations yield from inside context managers, so we have to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-4c70fa606777>\u001b[0m in \u001b[0;36mf\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/core.py\u001b[0m in \u001b[0;36m__bool__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    525\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m   \u001b[0;32mdef\u001b[0m \u001b[0m__bool__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    528\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__int__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__long__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_long\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/core.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, arg)\u001b[0m\n\u001b[1;32m    908\u001b[0m                       f\"or `jnp.array(x, {fun.__name__})` instead.\")\n\u001b[1;32m    909\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m     \u001b[0mraise_concretization_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mraise_concretization_error\u001b[0;34m(val, context)\u001b[0m\n\u001b[1;32m    897\u001b[0m          \u001b[0;34m\"See https://jax.readthedocs.io/en/latest/faq.html#abstract-tracer-value-encountered-where-concrete-value-is-expected-error for more information.\\n\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m           f\"Encountered tracer value: {val}\")\n\u001b[0;32m--> 899\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mConcretizationTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    900\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mConcretizationTypeError\u001b[0m: Abstract tracer value encountered where concrete value is expected.\n\nThe problem arose with the `bool` function. \n\n\n\nSee https://jax.readthedocs.io/en/latest/faq.html#abstract-tracer-value-encountered-where-concrete-value-is-expected-error for more information.\n\nEncountered tracer value: Traced<ShapedArray(bool[])>with<BatchTrace(level=1/0)>\n  with val = DeviceArray([False,  True,  True,  True,  True], dtype=bool)\n       batch_dim = 0"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-L-dB_GBWNq"
      },
      "source": [
        "## Side effects"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2zrYp3-KcMc"
      },
      "source": [
        "Since the jaxpr is created only once, if your function has global side-effects, such as using print, they will only happen once, even if the function is called multiple times. See example below.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LC5q9OCSTAKQ",
        "outputId": "0fbef0a2-647f-45fb-b5f0-c315a9372142"
      },
      "source": [
        "def f(x):\n",
        "  print('x', x)\n",
        "  y = 2 * x\n",
        "  print('y', y)\n",
        "  return y\n",
        "\n",
        "y1 = f(2)\n",
        "print('f', y1)\n",
        "print('\\ncall function a second time')\n",
        "y1 = f(2)\n",
        "print('f', y1)\n",
        "\n",
        "print('\\njit version follows')\n",
        "@jit\n",
        "def f(x):\n",
        "  print('x', x)\n",
        "  y = 2 * x\n",
        "  print('y', y)\n",
        "  return y\n",
        "y2 = f(2)\n",
        "print('f', y2)\n",
        "\n",
        "print('\\ncall jitted function a second time')\n",
        "y2 = f(2)\n",
        "print('f', y2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x 2\n",
            "y 4\n",
            "f 4\n",
            "\n",
            "call function a second time\n",
            "x 2\n",
            "y 4\n",
            "f 4\n",
            "\n",
            "jit version follows\n",
            "x Traced<ShapedArray(int32[], weak_type=True)>with<DynamicJaxprTrace(level=0/1)>\n",
            "y Traced<ShapedArray(int32[])>with<DynamicJaxprTrace(level=0/1)>\n",
            "f 4\n",
            "\n",
            "call jitted function a second time\n",
            "f 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFsfzYHzhbM3"
      },
      "source": [
        "## Caching\n",
        "\n",
        "If you write `f=jax.jit(g)`, then g will get compiled and the XLA code will be cahced. Subsequent calls to f reuse the cached code for speed. But if the jit is called inside a loop, it is effectively making a new f each time, which is slow.\n",
        "\n",
        "Also, if you specify static_argnums,hen the cached code will be used only for the same values of arguments labelled as static. If any of them change, recompilation occurs. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfZAt_DANKlW"
      },
      "source": [
        "## Strings\n",
        "\n",
        "Jit does not work with functions that consume or return strings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "id": "rlw0wY3YNPPV",
        "outputId": "628eee8f-dee9-4ec7-eac5-6d917c982bcf"
      },
      "source": [
        "def f(x: int, y: str):\n",
        "  if y=='add':\n",
        "    return x+1\n",
        "  else:\n",
        "    return x-1\n",
        "\n",
        "print(f(42, 'add'))\n",
        "print(f(42, 'sub'))\n",
        "\n",
        "fj = jax.jit(f)\n",
        "print(fj(42, 'add'))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "43\n",
            "41\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-89-613837d92373>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mfj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'add'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/api.py\u001b[0m in \u001b[0;36mf_jitted\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcache_miss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# probably won't return\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mcpp_jitted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m   \u001b[0mf_jitted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cpp_jitted_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcpp_jitted_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/api.py\u001b[0m in \u001b[0;36mcache_miss\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs_flat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m       \u001b[0m_check_arg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m     \u001b[0mflat_fun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflatten_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_tree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     out_flat = xla.xla_call(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/api.py\u001b[0m in \u001b[0;36m_check_arg\u001b[0;34m(arg)\u001b[0m\n\u001b[1;32m   2174\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTracer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_valid_jaxtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2175\u001b[0m     raise TypeError(\"Argument '{}' of type {} is not a valid JAX type\"\n\u001b[0;32m-> 2176\u001b[0;31m                     .format(arg, type(arg)))\n\u001b[0m\u001b[1;32m   2177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2178\u001b[0m \u001b[0;31m# TODO(necula): this duplicates code in core.valid_jaxtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Argument 'add' of type <class 'str'> is not a valid JAX type"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gctKo1NjJzQ1"
      },
      "source": [
        "# Pytrees\n",
        "\n",
        "A Pytree is a container of leaf elements and/or more pytrees. Containers include lists, tuples, and dicts. A leaf element is anything that’s not a pytree, e.g. an array.\n",
        "Pytrees are useful for representing hierarchical sets of parameters for DNNs (and other structured dsta). \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGxWcHWJKtsq"
      },
      "source": [
        "## Simple example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1EllUvlJ1NW",
        "outputId": "d53e7e75-e6c5-4d23-b5fc-2d66cb876632"
      },
      "source": [
        "from jax import tree_util\n",
        "\n",
        "# a simple pytree\n",
        "t1 = [1, {\"k1\": 2, \"k2\": (3, 4)}, 5]\n",
        "print('tree', t1)\n",
        "leaves = jax.tree_leaves(t1)\n",
        "print('num leaves', len(leaves))\n",
        "\n",
        "t4 = [jnp.array([1, 2, 3]), \"foo\"]\n",
        "print('tree', t4)\n",
        "leaves = jax.tree_leaves(t4)\n",
        "print('num leaves', len(leaves))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tree [1, {'k1': 2, 'k2': (3, 4)}, 5]\n",
            "num leaves 5\n",
            "tree [DeviceArray([1, 2, 3], dtype=int32), 'foo']\n",
            "num leaves 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zA0CvQoX4EiW"
      },
      "source": [
        "## Treemap\n",
        "\n",
        "\n",
        "We can map functions down a pytree in the same way that we can map a function down a list. We can also combine elements in two pytrees that have the same shape to make a third pytree."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0y3UFtJZKr08",
        "outputId": "356275b8-39a7-4a8a-b35a-5149fd102104"
      },
      "source": [
        "t1 = [1, {\"k1\": 2, \"k2\": (3, 4)}, 5]\n",
        "print(t1)\n",
        "\n",
        "t2 = tree_util.tree_map(lambda x: x*x, t1)\n",
        "print('square each element', t2)\n",
        "\n",
        "\n",
        "t3 = tree_util.tree_multimap(lambda x,y: x+y, t1, t2)\n",
        "print('t1+t2', t3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, {'k1': 2, 'k2': (3, 4)}, 5]\n",
            "square each element [1, {'k1': 4, 'k2': (9, 16)}, 25]\n",
            "t1+t2 [2, {'k1': 6, 'k2': (12, 20)}, 30]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpxCulXHEQ1_"
      },
      "source": [
        "If we have a list of dicts, we can convert to a dict of lists, as shown below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9L9EaaLBrPp",
        "outputId": "6400fe69-0a4a-4346-cca4-56f7890107bb"
      },
      "source": [
        "\n",
        "data = [dict(t=1, obs='a', val=-1), dict(t=2, obs='b', val=-2), dict(t=3, obs='c', val=-3)]\n",
        "\n",
        "data2 = jax.tree_multimap(lambda d0, d1, d2: list((d0, d1, d2)),\n",
        "                          data[0], data[1], data[2])\n",
        "print(data2)\n",
        "\n",
        "def join_trees(list_of_trees):\n",
        "  d = jax.tree_multimap(lambda *xs: list(xs), *list_of_trees)\n",
        "  return d\n",
        "\n",
        "print(join_trees(data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'obs': ['a', 'b', 'c'], 't': [1, 2, 3], 'val': [-1, -2, -3]}\n",
            "{'obs': ['a', 'b', 'c'], 't': [1, 2, 3], 'val': [-1, -2, -3]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvKtqaSu9xj2"
      },
      "source": [
        "## Flattening\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-ixjjrE90BF",
        "outputId": "217bd1c6-6bdb-42bd-8868-f0d528b488ad"
      },
      "source": [
        "\n",
        "t1 = [1, {\"k1\": 2, \"k2\": (3, 4)}, 5]\n",
        "leaves, foo = jax.tree_util.tree_flatten(t1)\n",
        "print(leaves)\n",
        "print(foo)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 2, 3, 4, 5]\n",
            "PyTreeDef(list, [*,PyTreeDef(dict[['k1', 'k2']], [*,PyTreeDef(tuple, [*,*])]),*])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8iMrvUXK8Id"
      },
      "source": [
        "## Example: Linear regression \n",
        "\n",
        "In this section we show how to use pytrees as a container for parameters of a linear reregression model. \n",
        "The code is based on the [flax JAX tutorial](https://flax.readthedocs.io/en/latest/notebooks/jax_for_the_impatient.html). When we compute the gradient, it will also be a pytree, and will have the same shape as the parameters, so we can add the params to the gradient without having to flatten and unflatten the parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77GAXfBmLByE"
      },
      "source": [
        "\n",
        "\n",
        "# Create the predict function from a set of parameters\n",
        "def make_predict_pytree(params):\n",
        "  def predict(x):\n",
        "    return jnp.dot(params['W'],x)+params['b']\n",
        "  return predict\n",
        "\n",
        "# Create the loss from the data points set\n",
        "def make_mse_pytree(x_batched,y_batched): # returns fn(params)->real\n",
        "  def mse(params):\n",
        "    # Define the squared loss for a single pair (x,y)\n",
        "    def squared_error(x,y):\n",
        "      y_pred = make_predict_pytree(params)(x)\n",
        "      return jnp.inner(y-y_pred,y-y_pred)/2.0\n",
        "    # We vectorize the previous to compute the average of the loss on all samples.\n",
        "    return jnp.mean(jax.vmap(squared_error)(x_batched,y_batched), axis=0)\n",
        "  return jax.jit(mse) # And finally we jit the result."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nk7Y8gcM7uhk"
      },
      "source": [
        "# Set problem dimensions\n",
        "N = 20\n",
        "xdim = 10\n",
        "ydim = 5\n",
        "\n",
        "# Generate random ground truth W and b\n",
        "key = random.PRNGKey(0)\n",
        "Wtrue = random.normal(key, (ydim, xdim))\n",
        "btrue = random.normal(key, (ydim,))\n",
        "params_true = {'W': Wtrue, 'b': btrue}\n",
        "true_predict_fun = make_predict_pytree(params_true)\n",
        "\n",
        "# Generate data with additional observation noise\n",
        "X = random.normal(key, (N, xdim))\n",
        "Ytrue = jax.vmap(true_predict_fun)(X)\n",
        "Y = Ytrue + 0.1*random.normal(key, (N, ydim))\n",
        "\n",
        "# Generate MSE for our samples\n",
        "mse_fun = make_mse_pytree(X, Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0YusY89LGS8",
        "outputId": "559cf97a-91fc-47c7-d599-2b1df9a4463a"
      },
      "source": [
        "# Initialize estimated W and b with zeros.\n",
        "params = {'W': jnp.zeros_like(Wtrue), 'b': jnp.zeros_like(btrue)}\n",
        "\n",
        "mse_pytree = make_mse_pytree(X, Y)\n",
        "print(mse_pytree(params_true))\n",
        "print(mse_pytree(params))\n",
        "print(jax.grad(mse_pytree)(params))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.022292046\n",
            "24.97824\n",
            "{'W': DeviceArray([[-0.039,  0.755,  0.542,  0.36 ,  0.224,  1.651,  1.534,\n",
            "              -1.342, -0.15 , -1.638],\n",
            "             [-0.324,  0.141, -0.402,  0.498,  1.829,  4.308,  2.138,\n",
            "              -2.43 , -0.381, -2.178],\n",
            "             [ 1.7  , -0.707, -0.656, -0.568,  1.824, -2.194, -0.477,\n",
            "               0.96 ,  1.622,  1.408],\n",
            "             [-0.862,  0.321, -0.388, -0.74 , -0.82 ,  0.441,  0.772,\n",
            "              -1.713, -1.592, -0.557],\n",
            "             [ 1.338, -0.632, -0.968, -1.127,  1.775,  0.323,  1.405,\n",
            "              -0.638,  1.077, -0.739]], dtype=float32), 'b': DeviceArray([ 0.036,  1.092, -0.413, -1.389, -0.862], dtype=float32)}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WA0ZjMofMFW6",
        "outputId": "db054c94-a79e-4cd5-92fc-e1fe8fad97c4"
      },
      "source": [
        "alpha = 0.3 # Gradient step size\n",
        "print('Loss for \"true\" W,b: ', mse_pytree(params_true))\n",
        "for i in range(101):\n",
        "  gradients = jax.grad(mse_pytree)(params)\n",
        "  params = jax.tree_multimap(lambda old,grad: old-alpha*grad, params, gradients)\n",
        "  if (i%10==0):\n",
        "    print(\"Loss step {}: \".format(i), mse_pytree(params))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss for \"true\" W,b:  0.022292046\n",
            "Loss step 0:  6.559746\n",
            "Loss step 10:  0.17232792\n",
            "Loss step 20:  0.04339735\n",
            "Loss step 30:  0.024473606\n",
            "Loss step 40:  0.017078906\n",
            "Loss step 50:  0.013489485\n",
            "Loss step 60:  0.011695381\n",
            "Loss step 70:  0.01079526\n",
            "Loss step 80:  0.010343453\n",
            "Loss step 90:  0.010116666\n",
            "Loss step 100:  0.0100028105\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_tNi1V38VAo",
        "outputId": "28ff2151-6341-4a9f-a39f-242ce46e0c53"
      },
      "source": [
        "print(jax.tree_multimap(lambda x,y: np.allclose(x,y, atol=1e-1), params, params_true))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'W': True, 'b': True}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfHqKwgl9T2Q"
      },
      "source": [
        "Compare the above to what the training code would look like\n",
        "if W and b were passed in as separate arguments:\n",
        "```\n",
        "for i in range(101):\n",
        "  grad_W = jax.grad(mse_fun,0)(What,bhat)\n",
        "  grad_b = jax.grad(mse_fun,1)(What,bhat)\n",
        "  What = What - alpha*grad_W\n",
        "  bhat = bhat - alpha*grad_b \n",
        "  if (i%10==0):\n",
        "    print(\"Loss step {}: \".format(i), mse_fun(What,bhat)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2t_pU8lG9xyl"
      },
      "source": [
        "## Example: MLPs\n",
        "\n",
        "We now show a more interesting example, from the Deepmind tutorial, where we fit an MLP using SGD. The basic structure is similar to the linear regression case.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8FyAwlf9_Vk"
      },
      "source": [
        "# define the model \n",
        "def init_mlp_params(layer_widths):\n",
        "  params = []\n",
        "  for n_in, n_out in zip(layer_widths[:-1], layer_widths[1:]):\n",
        "    params.append(\n",
        "        dict(weights=np.random.normal(size=(n_in, n_out)) * np.sqrt(2/n_in),\n",
        "             biases=np.ones(shape=(n_out,))\n",
        "            )\n",
        "    )\n",
        "  return params\n",
        "\n",
        "def forward(params, x):\n",
        "  *hidden, last = params\n",
        "  for layer in hidden:\n",
        "    x = jax.nn.relu(x @ layer['weights'] + layer['biases'])\n",
        "  return x @ last['weights'] + last['biases']\n",
        "\n",
        "def loss_fn(params, x, y):\n",
        "  return jnp.mean((forward(params, x) - y) ** 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hx5EXL79-B6i",
        "outputId": "313e9984-8a3a-4a30-f0ef-aab7d8c2a194"
      },
      "source": [
        "# MLP with 2 hidden layers and linear output\n",
        "np.random.seed(0)\n",
        "params = init_mlp_params([1, 128, 128, 1])\n",
        "jax.tree_map(lambda x: x.shape, params)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'biases': (128,), 'weights': (1, 128)},\n",
              " {'biases': (128,), 'weights': (128, 128)},\n",
              " {'biases': (1,), 'weights': (128, 1)}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "ITNC0muv-RiH",
        "outputId": "5056a295-5d44-4b2e-a192-97daf524298e"
      },
      "source": [
        "LEARNING_RATE = 0.0001\n",
        "\n",
        "@jax.jit\n",
        "def update(params, x, y):\n",
        "  grads = jax.grad(loss_fn)(params, x, y)\n",
        "  return jax.tree_multimap(\n",
        "      lambda p, g: p - LEARNING_RATE * g, params, grads)\n",
        "\n",
        "np.random.seed(0)\n",
        "xs = np.random.normal(size=(200, 1))\n",
        "ys = xs ** 2\n",
        "\n",
        "for _ in range(1000):\n",
        "  params = update(params, xs, ys)\n",
        "\n",
        "plt.scatter(xs, ys, label='truth')\n",
        "plt.scatter(xs, forward(params, xs), label='Prediction')\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f22162c82e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDXJO4FdUPJM"
      },
      "source": [
        "# Looping constructs\n",
        "\n",
        "For loops in Python are slow, even when JIT-compiled. However, there are built-in primitives for loops that are fast, as we illustrate below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-wtZiZmU329"
      },
      "source": [
        "## For loops.\n",
        "\n",
        "The semantics of the for loop function in JAX is as follows:\n",
        "```\n",
        "def fori_loop(lower, upper, body_fun, init_val):\n",
        "  val = init_val\n",
        "  for i in range(lower, upper):\n",
        "    val = body_fun(i, val)\n",
        "  return val\n",
        "```\n",
        "We see that ```val``` is used to accumulate the results across iterations.\n",
        "\n",
        "Below is an example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIlLD0kKVGya"
      },
      "source": [
        "# sum from 1 to N = N*(N+1)/2\n",
        "\n",
        "def sum_exact(N):\n",
        "  return int(N*(N+1)/2)\n",
        "\n",
        "def sum_slow(N):\n",
        "  s = 0\n",
        "  for i in range(1,N+1):\n",
        "    s += i\n",
        "  return s\n",
        "\n",
        "N = 10\n",
        "\n",
        "assert sum_slow(N) == sum_exact(N)\n",
        "\n",
        "def sum_fast(N):\n",
        "  s = jax.lax.fori_loop(1, N+1, lambda i,partial_sum: i+partial_sum, 0)\n",
        "  return s\n",
        "\n",
        "assert sum_fast(N) == sum_exact(N) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWuOBk9SRLJb",
        "outputId": "b3cf35ae-b82b-461b-8ea4-86cc7fb93575"
      },
      "source": [
        "N = 1000\n",
        "%timeit sum_slow(N)\n",
        "%timeit sum_fast(N)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000 loops, best of 3: 44.1 µs per loop\n",
            "10 loops, best of 3: 41 ms per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSH6VQLHRWfA",
        "outputId": "01bd0110-0e55-4038-d04f-9074741e71c5"
      },
      "source": [
        "N = 100000\n",
        "%timeit sum_slow(N)\n",
        "%timeit sum_fast(N)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100 loops, best of 3: 5.04 ms per loop\n",
            "1 loop, best of 3: 2.88 s per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvUsG4mUR3Sl"
      },
      "source": [
        "# Let's do more compute per step of the for loop\n",
        "\n",
        "D = 10\n",
        "X = jax.random.normal(key, shape=(D,D))\n",
        "\n",
        "def sum_slow(N):\n",
        "  s = jnp.zeros_like(X)\n",
        "  for i in range(1,N+1):\n",
        "    s += jnp.dot(X, X)\n",
        "  return s\n",
        "\n",
        "def sum_fast(N):\n",
        "  s = jnp.zeros_like(X)\n",
        "  s = jax.lax.fori_loop(1, N+1, lambda i,s: s+jnp.dot(X,X), s)\n",
        "  return s\n",
        "\n",
        "N = 10\n",
        "assert np.allclose(sum_fast(N), sum_slow(N))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8NtevKvS8F_",
        "outputId": "a330fe55-f3c9-4854-90f5-f177541f5b66"
      },
      "source": [
        "N = 1000\n",
        "%timeit sum_slow(N)\n",
        "%timeit sum_fast(N)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 loop, best of 3: 482 ms per loop\n",
            "10 loops, best of 3: 46.3 ms per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ru5D0tZEiV2e"
      },
      "source": [
        "## While loops\n",
        "\n",
        "Here is the semantics of the JAX while loop\n",
        "\n",
        "\n",
        "```\n",
        "def while_loop(cond_fun, body_fun, init_val):\n",
        "  val = init_val\n",
        "  while cond_fun(val):\n",
        "    val = body_fun(val)\n",
        "  return val\n",
        "```\n",
        "\n",
        "Below is an example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWKhfJTDiiSI"
      },
      "source": [
        "\n",
        "\n",
        "def sum_slow_while(N):\n",
        "  s = 0\n",
        "  i = 0\n",
        "  while (i <= N):\n",
        "    s += i\n",
        "    i += 1\n",
        "  return s\n",
        "\n",
        "\n",
        "def sum_fast_while(N):\n",
        "  init_val = (0,0)\n",
        "  def cond_fun(val):\n",
        "    s,i = val\n",
        "    return i<=N\n",
        "  def body_fun(val):\n",
        "    s,i = val\n",
        "    s += i\n",
        "    i += 1\n",
        "    return (s,i)\n",
        "  val = jax.lax.while_loop(cond_fun, body_fun, init_val)\n",
        "  s2 = val[0]\n",
        "  return s2\n",
        "\n",
        "N = 10\n",
        "assert sum_slow_while(N) == sum_exact(N)\n",
        "assert sum_slow_while(N) == sum_fast_while(N)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCOVFuZg2QKm"
      },
      "source": [
        "# Common gotchas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JbntckLIflT"
      },
      "source": [
        "## Handling state\n",
        "\n",
        "In this section, we discuss how to transform code that uses object-oriented programming (which can be stateful) to pure functional programming, which is stateless, as required by JAX. Our presentation is based on the Deepmind tutorial.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xhaFAPgJUZl"
      },
      "source": [
        "To start, consider a simple class that maintains an internal counter, and when called, increments the counter and returns the next number from some sequence. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIv0ke8VJXjf",
        "outputId": "534d6510-c218-4f3b-c6e7-42c4dd65c199"
      },
      "source": [
        "#import string\n",
        "#DICTIONARY = list(string.ascii_lowercase)\n",
        "SEQUENCE = jnp.arange(0,100,2)\n",
        "\n",
        "class Counter:\n",
        "\n",
        "  def __init__(self):\n",
        "    self.n = 0\n",
        "\n",
        "  def count(self) -> int:\n",
        "    #res = DICTIONARY[self.n]\n",
        "    res = SEQUENCE[self.n]\n",
        "    self.n += 1\n",
        "    return res\n",
        "\n",
        "  def reset(self):\n",
        "    self.n = 0\n",
        "\n",
        "\n",
        "counter = Counter()\n",
        "\n",
        "for _ in range(3):\n",
        "  print(counter.count())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "2\n",
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc5RW-O0LIhy"
      },
      "source": [
        "The trouble with the above code is that the call to `count` depends on the internal state of the object (the value `n`), even though this is not an argument to the function. (The code is therefoe said to violate 'referential transparency'.) When we Jit compile it, Jax will only call the code once (to convert to a jaxpr), so the side effect of updating `n` will not happen, resulting in incorrect behavior, as we show below,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPFnq8ufLjxM",
        "outputId": "a5c06352-feac-4ca4-9059-87f5f9163c15"
      },
      "source": [
        "counter.reset()\n",
        "fast_count = jax.jit(counter.count)\n",
        "\n",
        "for _ in range(3):\n",
        "  print(fast_count())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "0\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUeUHAKMMrCv"
      },
      "source": [
        "We can solve this problem by passing the state as an argument into the function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spj-8wELMzJV",
        "outputId": "20166198-4315-4300-c4d4-789d35e409f5"
      },
      "source": [
        "CounterState = int\n",
        "Result = int\n",
        "\n",
        "class CounterV2:\n",
        "\n",
        "  def count(self, n: CounterState) -> Tuple[Result, CounterState]:\n",
        "    return SEQUENCE[n], n+1\n",
        "\n",
        "  def reset(self) -> CounterState:\n",
        "    return 0\n",
        "\n",
        "counter = CounterV2()\n",
        "state = counter.reset()\n",
        "\n",
        "for _ in range(3):\n",
        "  value, state = counter.count(state)\n",
        "  print(value)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "2\n",
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xzy9QtLnNy74"
      },
      "source": [
        "This version is functionally pure, so jit-compiles nicely."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGbuYX8MN3mN",
        "outputId": "d876823b-12fc-4dfd-80b4-857e873dac8a"
      },
      "source": [
        "state = counter.reset()\n",
        "fast_count = jax.jit(counter.count)\n",
        "\n",
        "for _ in range(3):\n",
        "  value, state = fast_count(state)\n",
        "  print(value)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "2\n",
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9jmTUu5ORwB"
      },
      "source": [
        "\n",
        "We can apply the same process to any stateful method to convert it into a stateless one. We took a class of the form\n",
        "\n",
        "```\n",
        "class StatefulClass\n",
        "\n",
        "  state: State\n",
        "\n",
        "  def stateful_method(*args, **kwargs) -> Output:\n",
        "```\n",
        "\n",
        "and turned it into a class of the form\n",
        "\n",
        "```\n",
        "class StatelessClass\n",
        "\n",
        "  def stateless_method(state: State, *args, **kwargs) -> (Output, State):\n",
        "```\n",
        "\n",
        "This is a common [functional programming](https://en.wikipedia.org/wiki/Functional_programming) pattern, and, essentially, is the way that state is handled in all JAX programs (as we saw with the way Jax handles random number state, or parameters of a model that get updated).\n",
        "Note that the stateless version of the code no longer needs to use a class, but can instead group the functions into a common namespace using modules.\n",
        "\n",
        "In some cases (eg when working with DNNs), it is more convenient to write code in an OO way. There are several libraries (notably [Flax](https://github.com/google/flax) and [Haiku](https://github.com/deepmind/dm-haiku))  that let you define a model in an OO way, and then generate functionally pure code. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AieugSOZLGi5"
      },
      "source": [
        "## Mutation of arrays \n",
        "\n",
        "Since JAX is functional, you cannot mutate arrays in place,\n",
        "since this makes program analysis and transformation very difficult. JAX requires a pure functional expression of a numerical program.\n",
        "Instead, JAX offers the functional update functions: `index_update`, `index_add`, `index_min`, `index_max`, and the `index` helper. These are illustrated below. \n",
        "\n",
        "Note: If the input values of `index_update` aren't reused, jit-compiled code will perform these operations in-place, rather than making a copy. \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEKfhvrTLEBf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c263bc2-4431-48ee-b805-c89e4bb47b87"
      },
      "source": [
        "# You cannot assign directly to elements of an array.\n",
        "\n",
        "A = jnp.zeros((3,3), dtype=np.float32)\n",
        "\n",
        "# In place update of JAX's array will yield an error!\n",
        "try:\n",
        "  A[1, :] = 1.0\n",
        "except:\n",
        "  print('must use index_update')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "must use index_update\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9P3uUMzXVjw9",
        "outputId": "0dfc73a6-5d63-4364-a0eb-e966bb821b01"
      },
      "source": [
        "from jax.ops import index, index_add, index_update\n",
        "\n",
        "D = 3\n",
        "A = 2*jnp.ones((D,D))\n",
        "print(\"original array:\")\n",
        "print(A)\n",
        "\n",
        "A2 = index_update(A, index[1, :], 42.0) # A[1,:] = 42\n",
        "print(\"original array:\")\n",
        "print(A) # unchanged\n",
        "print(\"new array:\")\n",
        "print(A2)\n",
        "\n",
        "A3 = A.at[1,:].set(42.0) # A3=np.copy(A),  A3[1,:] = 42\n",
        "print(\"original array:\")\n",
        "print(A) # unchanged\n",
        "print(\"new array:\")\n",
        "print(A3)\n",
        "\n",
        "A4 = A.at[1,:].mul(42.0) # A4=np.copy(A),  A4[1,:] *= 42\n",
        "print(\"original array:\")\n",
        "print(A) # unchanged\n",
        "print(\"new array:\")\n",
        "print(A4)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original array:\n",
            "[[2. 2. 2.]\n",
            " [2. 2. 2.]\n",
            " [2. 2. 2.]]\n",
            "original array:\n",
            "[[2. 2. 2.]\n",
            " [2. 2. 2.]\n",
            " [2. 2. 2.]]\n",
            "new array:\n",
            "[[ 2.  2.  2.]\n",
            " [42. 42. 42.]\n",
            " [ 2.  2.  2.]]\n",
            "original array:\n",
            "[[2. 2. 2.]\n",
            " [2. 2. 2.]\n",
            " [2. 2. 2.]]\n",
            "new array:\n",
            "[[ 2.  2.  2.]\n",
            " [42. 42. 42.]\n",
            " [ 2.  2.  2.]]\n",
            "original array:\n",
            "[[2. 2. 2.]\n",
            " [2. 2. 2.]\n",
            " [2. 2. 2.]]\n",
            "new array:\n",
            "[[ 2.  2.  2.]\n",
            " [84. 84. 84.]\n",
            " [ 2.  2.  2.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwmI9DH2K_nl"
      },
      "source": [
        "## Implicitly casting lists to vectors\n",
        "\n",
        "You cannot treat a list of numbers as a vector. Instead you must explicitly create the vector using the np.array() constructor.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnOUiLeA1kBo"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUURw01jKnXC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1884649d-788a-4533-9bc3-2813b23e55f7"
      },
      "source": [
        "# You cannot treat a list of numbers as a vector. \n",
        "try:\n",
        "  S = jnp.diag([1.0, 2.0, 3.0])\n",
        "except:\n",
        "  print('must convert indices to np.array')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "must convert indices to np.array\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mMgaegpLCYw"
      },
      "source": [
        "# Instead you should explicitly construct the vector.\n",
        "\n",
        "S = jnp.diag(jnp.array([1.0, 2.0, 3.0]))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}