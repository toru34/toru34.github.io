{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "layer-norm-torch.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMG1mBr0d4qvpByDjLD3+XP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/probml/pyprobml/blob/master/notebooks/layer_norm_torch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxxCjM4AIsZP"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "np.random.seed(seed=1)\n",
        "import math\n",
        "import collections\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcFY2bZeImQI",
        "outputId": "bbfe5c4d-b5d4-4657-a446-269662828f4e"
      },
      "source": [
        "\n",
        "# batch size 3, feature size 2\n",
        "X = np.array([[1, 2, 3], [4, 5, 6]])\n",
        "#X = np.array([[1, 2], [2,3]], dtype=np.float32)\n",
        "\n",
        "print('batch norm')\n",
        "mu_batch = np.mean(X,axis=0)\n",
        "sigma_batch = np.std(X,axis=0)\n",
        "XBN = (X-mu_batch)/sigma_batch\n",
        "print(XBN)\n",
        "\n",
        "print('layer norm')\n",
        "mu_layer = np.expand_dims(np.mean(X,axis=1),axis=1)\n",
        "sigma_layer = np.expand_dims(np.std(X,axis=1), axis=1)\n",
        "XLN = (X-mu_layer)/sigma_layer\n",
        "print(XLN)\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "batch norm\n",
            "[[-1. -1. -1.]\n",
            " [ 1.  1.  1.]]\n",
            "layer norm\n",
            "[[-1.22474487  0.          1.22474487]\n",
            " [-1.22474487  0.          1.22474487]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6p9f1kSyJbuT",
        "outputId": "c8344ff2-629b-4976-bfaa-bd65663f047b"
      },
      "source": [
        "\n",
        "X = torch.tensor(X, dtype=torch.float32)\n",
        "N, D = X.shape\n",
        "\n",
        "ln = nn.LayerNorm(D)\n",
        "bn = nn.BatchNorm1d(D)\n",
        "\n",
        "print('batch norm')\n",
        "XBN_t = bn(X)\n",
        "print(XBN_t)\n",
        "assert(np.allclose(XBN_t.detach().numpy(), XBN, atol=1e-3))\n",
        "\n",
        "print('layer norm')\n",
        "XLN_t = ln(X)\n",
        "print(XLN_t)\n",
        "assert(np.allclose(XLN_t.detach().numpy(), XLN, atol=1e-3))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "batch norm\n",
            "tensor([[-1.0000, -1.0000, -1.0000],\n",
            "        [ 1.0000,  1.0000,  1.0000]], grad_fn=<NativeBatchNormBackward>)\n",
            "layer norm\n",
            "tensor([[-1.2247e+00,  0.0000e+00,  1.2247e+00],\n",
            "        [-1.2247e+00,  1.1921e-07,  1.2247e+00]],\n",
            "       grad_fn=<NativeLayerNormBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQU6R65hJvtE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}